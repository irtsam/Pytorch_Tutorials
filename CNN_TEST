#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Mar 26 15:15:41 2018

@author: ighazi
"""
import torch
import torch.nn as nn
import torch.nn.functional as f
import numpy as np
import torch.optim as optim
from torch.autograd import Variable
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import time




# functions to show an image


def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))


# get some random training images



"""This code is for creating a convolutional neural network classifier that 
will work with Pascal VOC to classify our images"""

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        """The following are the two convolutional layers and the first one 
        comprises of 1 input channel i.e. the images, 6 output channel and 
        convolutional kernel of sie 5"""
        self.conv1= nn.Conv2d(3,6,5)
        """Input channels of size 6, output channels of size 16 and kernel of
        size 5"""
        self.conv2= nn.Conv2d(6,16,5)
        #self.conv2_drop = nn.Dropout2d()
        """Fully connected neural network portion"""
        self.fc1= nn.Linear(16*5*5, 120)
        self.fc2= nn.Linear(120, 84)
        
        self.fc3= nn.Linear(84, 10)
       
        
        
        """Thus our neural network parameters have been defined"""
    def forward(self,x):
        """The x will be the output vector to feed forward the network. we 
        perform the RelU portion and feeding forward based on our input over
        here as well"""
        
        """Below is the feed forward to convolve, apply relu and Max_pool our code"""        
        x= f.max_pool2d(f.relu(self.conv1(x) ), (2,2))
        #print(x.size())
        """The next convolutional layer again maxpool size is 2*2"""
        x= f.max_pool2d(f.relu(self.conv2(x)),2)
        """Now we flatten our convolved output as a row"""
        #print(x.size())
        #x = f.dropout(x, training=self.training)
        x= x.view(-1, self.num_flat_features(x))
        """The reLU portion actions on the input depending on the object passed
        to it i.e acts both on convolutional objects and linear objects"""
        
        x= f.relu(self.fc1(x))
        #print(x.size())
        
        x= f.relu(self.fc2(x))
        #x = f.dropout(x, p=0.1,inplace= True)
        #print(x.size())
        x= self.fc3(x)
     
        
        return x
        
    def num_flat_features(self,x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        #print(size)
        num_features = 1
        for s in size:
            num_features *= s
        #print(num_features)
        return num_features

"""Create a neural net object and then feedforward it by using the net(x) 
procedure. This works as function is one of the abstract functions for nn.module
superclass we inherited"""
global x,y

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)
start_time= time.time()
classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')


dataiter = iter(trainloader)
images, labels = dataiter.next()
net = Net()
# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9,weight_decay=5e-4)


"""Training our network"""

for epoch in range(200):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data

        # wrap them in Variable
        inputs, labels = Variable(inputs), Variable(labels)

        # zero the parameter gradients
        optimizer.zero_grad()
        
        #net=nn.Dropout(p=0.5, inplace= True)
        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.data[0]
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')


"""Testing our data"""

correct = 0
total = 0
for data in trainloader:
    images, labels = data
    outputs = net(Variable(images))
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))


"""Classes Prediction Accuracy"""
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
for data in testloader:
    images, labels = data
    outputs = net(Variable(images))
    _, predicted = torch.max(outputs.data, 1)
    c = (predicted == labels).squeeze()
    for i in range(4):
        label = labels[i]
        class_correct[label] += c[i]
        class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))


tot_time= time.time()- start_time
print(tot_time)

























"""
#Defining a random input
x= Variable(torch.rand(1,1,32,32), requires_grad= True)
out= net(x)
print(net)
parameters= list(net.parameters())

print(parameters[0])

g= torch.randn(1,10)
net.zero_grad()
out.backward(torch.randn(1,10))
print(g)
        
Backpropagation"""

"""Backpropagation practice
input = Variable(torch.randn(1, 1, 32, 32))
out = net(input)
print(out)

net.zero_grad()
out.backward(torch.randn(1, 10))
print(out.grad)
target = Variable(torch.arange(1, 11))

optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
criterion = nn.MSELoss()
loss = criterion(output, target)
loss.backward(retain_graph=True)
print(loss)
optimizer.step() 
loss.backward()
print(loss)
optimizer.step()
"""